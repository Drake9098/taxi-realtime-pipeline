# üöñ Real-Time Taxi Demand Prediction (End-to-End MLOps)

A complete **End-to-End MLOps system** that simulates a real-time streaming pipeline to predict the duration of NYC taxi trips. The system continuously learns from new incoming data (**Continuous Training**) and exposes an API for real-time predictions.

## üìã Project Status

üü¢ **Status:** Completed

- [x] **Data Ingestion:** Scalable Kafka Producer (Python + Docker).
- [x] **Streaming Infrastructure:** Apache Kafka & Zookeeper.
- [x] **Continuous Training:** Consumer that trains Random Forest models on-the-fly.
- [x] **Model Registry:** Experiment tracking and artifact versioning with MLflow.
- [x] **Inference API:** FastAPI microservice with "Lazy Loading" pattern to serve the latest available model.

## üèóÔ∏è Monorepo Architecture

The project follows a **Microservices** architecture, orchestrated via Docker Compose.

```
taxi-realtime-pipeline/
‚îÇ
‚îú‚îÄ‚îÄ producer/ # üì° Service: Sends streaming data to Kafka
‚îÇ ‚îú‚îÄ‚îÄ app.py
‚îÇ ‚îî‚îÄ‚îÄ Dockerfile
‚îÇ
‚îú‚îÄ‚îÄ training/ # üéì Service: Consumes data & trains models
‚îÇ ‚îú‚îÄ‚îÄ train.py # Training logic & MLflow logging
‚îÇ ‚îî‚îÄ‚îÄ Dockerfile
‚îÇ
‚îú‚îÄ‚îÄ inference/ # üöÄ Service: Exposes REST API for predictions
‚îÇ ‚îú‚îÄ‚îÄ main.py # FastAPI app with auto-reload model logic
‚îÇ ‚îî‚îÄ‚îÄ Dockerfile
‚îÇ
‚îú‚îÄ‚îÄ data/ # üíæ Local data (Excluded from Git)
‚îú‚îÄ‚îÄ mlruns/ # üìÇ Shared volume for MLflow artifacts
‚îú‚îÄ‚îÄ docker-compose.yml # üê≥ Orchestration
‚îî‚îÄ‚îÄ README.md
```

## üéØ Technical Choices & Best Practices

Microservices Isolation: Each component runs in its own isolated Python environment (venv/Dockerfile) to avoid dependency conflicts.

Event-Driven: Decoupling between data production and model training via Kafka.

Data Robustness: Explicit type handling (Float64) and Schema Enforcement via MLflow Signatures.

API Resilience: Implementation of the Lazy Loading pattern in the Inference API to handle cold starts or temporary model unavailability gracefully.

## üöÄ Quick Start

### Prerequisites

Docker and Docker Compose installed.

Dataset: Download one of the "Yellow Taxi Trip Records" files (Parquet format) from the official NYC TLC website, rename it to taxi_data.parquet, and place it in the data/ folder.

### Launch

Start the entire infrastructure with a single command:

```
docker-compose up -d --build
```

This command will build and start the following services:

- Zookeeper
- Kafka Broker
- Kafka UI (for monitoring streams)
- Data Producer (sends taxi trip data to Kafka)
- Training Service (consumes data, trains models, logs to MLflow)
- Inference API (serves predictions via FastAPI)
- MLflow Tracking Server (for experiment tracking and model registry)
- MLflow UI (for visualizing experiments and models)

Wait approximately 60-90 seconds for the Training Service to collect the first batch of data (default: 10,000 records) and generate the initial model.

### Dashboards & Monitoring

Kafka UI: http://localhost:8080 (Stream monitoring)

MLflow UI: http://localhost:5000 (MAE metrics & Models visualization)

API Documentation: http://localhost:8000/docs (Swagger UI)

### Test Prediction (Inference)

You can test the API directly via Swagger UI or using curl in your terminal:

JSON Request Example (JFK Airport -> Times Square):

```
{
"PULocationID": 132,
"DOLocationID": 230,
"trip_distance": 18.5
}
```

Expected Response:

```
{
"predicted_duration_minutes": 51.64, (approximate value)
"ride_details": { ... }
}
```

## üõ†Ô∏è Tech Stack

**Streaming**: Apache Kafka, Zookeeper

**ML & Data**: Scikit-Learn, Pandas, MLflow

**Backend**: FastAPI, Uvicorn, Pydantic

**Containerization**: Docker, Docker Compose

**Language**: Python 3.11
